{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from datascrap.app import handler\n", "import pandas as pd\n", "import os"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this exercise, we'll map to the metadata of the papers dataset with the DOME dataset ontology"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def map_and_save_metadata(df, file_path, keyphrases=None, queries=None):\n", "    \"\"\"\n", "    Map the dataframe columns to the custom ontology headers and save it as a CSV file.\n", "    Args:\n", "        df (pd.DataFrame): The dataframe containing paper information.\n", "        file_path (str): The file path where the CSV will be saved.\n", "        keyphrases (list): A list of keyphrases (semantic keywords).\n", "        queries (dict): A dictionary of queries used (syntactic keywords).\n", "    \"\"\"\n", "    # Map the DataFrame columns to custom headers\n", "    df['has_dataset_creator'] = df['authors']  # Mapping authors to creator\n", "    df['has_dataset_license'] = ''  # Assuming no license information available\n", "    df['has_dataset_publisher'] = 'arXiv'  # Publisher set to arXiv\n", "    df['has_dataset_topic'] = df['categories']  # Mapping categories to topic\n", "    df['has_dataset_description'] = df['abstract']  # Mapping abstract to description\n", "    df['has_dataset_issued_date'] = df['year']  # Mapping year to issued date\n", "    df['has_dataset_semantic_keyword'] = ','.join(keyphrases) if keyphrases else ''  # Mapping keyphrases\n", "    df['has_syntactic_keyword'] = ','.join(queries.keys()) if queries else ''  # Mapping queries\n", "    df['has_dataset_title'] = df['title']  # Mapping title to dataset title\n\n", "    # Specify the custom columns order for the CSV file\n", "    custom_columns = [\n", "        'has_dataset_creator',\n", "        'has_dataset_license',\n", "        'has_dataset_publisher',\n", "        'has_dataset_topic',\n", "        'has_dataset_description',\n", "        'has_dataset_issued_date',\n", "        'has_dataset_semantic_keyword',\n", "        'has_syntactic_keyword',\n", "        'has_dataset_title'\n", "    ]\n\n", "    # Save the DataFrame with custom headers to CSV\n", "    df.to_csv(file_path, columns=custom_columns, index=False, encoding='utf-8')\n", "    print(f\"Metadata saved to {file_path}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["event = {\n", "    'keyphrases': 'nasicon',\n", "    'queries': {'DFT': 'ML'},\n", "    'num_results': 5\n", "}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["context = {}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["result = handler(event, context)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Convert the keyphrases and queries from the event"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["keyphrases = event['keyphrases'].split('-')\n", "queries = event['queries']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create a local folder to save the CSV"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["local_folder = './papers'\n", "os.makedirs(local_folder, exist_ok=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Save the mapped dataframe to CSV using the custom headers"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["metadata_file = os.path.join(local_folder, 'metadata_custom.csv')\n", "df = pd.DataFrame(result['dataframe'])  # Convert the result into a DataFrame\n", "map_and_save_metadata(df, metadata_file, keyphrases=keyphrases, queries=queries)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(result)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Part 2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This time we'll make a CUDS using the DOMEDS ontology"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from discomat.cuds.cuds import Cuds\n", "from discomat.visualisation.cuds_vis import gvis\n", "from discomat.ontology.namespaces import DOMEDS\n", "import csv"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we'll use del to create a dataset of the downlaoded papers"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load dataset from CSV"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with open('papers/metadata_custom.csv', mode='r') as file:\n", "    reader = csv.reader(file)\n", "    headers = next(reader)  # Read the headers\n\n", "    # Here we'll use list comprehension instead of creating each triple line by line\n\n", "    # Clean and map the headers to ontology fields\n", "    ontology_fields = [getattr(DOMEDS, header.strip()) for header in headers if header.strip()]\n\n", "    # Read the dataset rows\n", "    dataset = [tuple(row) for row in reader]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["gall = []"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Loop through the dataset and create CUDS objects"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for data in dataset:\n", "    ds_001 = Cuds(ontology_type=DOMEDS.data_set, description=\"Example of DOME4.0 dataset\")\n\n", "    # Use list comprehension to add each field to the CUDS object\n", "    [ds_001.add(field, value) for field, value in zip(ontology_fields, data)]\n", "    print(ds_001)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # accumulate the current dataset graph to gall\n", "    gall += ds_001.graph"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Visualize the combined graph and export to HTML"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["gvis(gall, \"papers.html\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now let's export the CUDS to .ttl and upload to DOME 4.0 platform"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from rdflib import Graph\n", "g_total = Graph()\n", "for g in gall:\n", "    g_total.add(g)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["g_total.serialize(\"datasets.ttl\", format=\"ttl\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}